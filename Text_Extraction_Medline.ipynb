{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and extract text information from Medline Abstracts\n",
    "\n",
    " \n",
    "### Purpose: \n",
    "    Analyze the corpus of medline abstracts and extract the text information\n",
    "\n",
    "### Input: \n",
    "    A cropus of text files with medline abstract. The first line is the title, and rest of the document is the abstract body.\n",
    "\n",
    "### Output:\n",
    "    1. Average words per document\n",
    "    2. Print total count of 26 alphabet characters in all of the documents\n",
    "    3. Print top ten rarely used words-sorted form least used word to most used word\n",
    "\n",
    "### Steps\n",
    "    Get list of documents\n",
    "### 1. Average words per documents\n",
    "    (1) Create abst_dict emplty dictionary.\n",
    "    (2) For each document, read the content of the file using read() and split the strings into a list of words using split()   method\n",
    "    (3) Use for  loop to go through word by word from the list and store value in the abst_dict and if word is already there, then add +1 in the value of key word.\n",
    "    (4) Assign 0 to \"sum\" variable and then using for loop extract value for each key from the dictionary and calculate average word per document.\n",
    "### 2. Total count of 26 alphabets characters in all of the documents\n",
    "    (1)Assign all 26 alphabets in \"alphabets\" varible and create empty \"alphabet_dict\" dictionary with keys from alphabets and value from 0.\n",
    "    (2)For each document, read the content of the file using read() and split the strings into a list of words using split()   method and convert all the word into uppercase using upper().\n",
    "    (3)Split each word into letters and stored into alphabet dictionary. Print the each key-value from the dictionary.\n",
    "### 3. Top ten rarely used words\n",
    "    (1)Create two empty dictionary. word_dict will store all the words and frequency and unique_dict will store unique frequency and word.\n",
    "    (2)For each document, read the content of the file using read() and split the strings into a list of words using split()   method\n",
    "    (3)Use for loop to go through word by word from the list and store value in the empty directory and if word is already there, then add +1 in the value of key word.\n",
    "    (4)Use for loop to go through each key-value pair in word_dict and extract unique value-key pair in unique_dict.\n",
    "    (5)Sort the unique_dict on ascending value and print first 10 key-value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required dependancies\n",
    "import os\n",
    "import string\n",
    "from string import ascii_letters\n",
    "\n",
    "#Create data directory and list all the medline abstracts\n",
    "abst_directory=\"pubmed/\"\n",
    "abst_list=os.listdir(abst_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Average words per document\n",
    "We are going to reach each file, and load abstract. Split the abstract into words and calculate avarage words per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 180.7 words/documents\n"
     ]
    }
   ],
   "source": [
    "# Create global variable-empty dictionary\n",
    "abst_dict = { }\n",
    "\n",
    "# Using for loop to go through each file to extract the information\n",
    "for file in abst_list:\n",
    "    with open(abst_directory + file, \"r\") as f: #Use of with open to read text file\n",
    "        for word in f.read().split():           #Using for loop to go through list\n",
    "            if word in abst_dict:               #If word is in the dictionary, then add +1 to previous key-value pair\n",
    "                abst_dict[word] += 1\n",
    "            else:                               #If word isnot in the dictionary, then assign +1 as a key-value pair\n",
    "                abst_dict[word] = 1\n",
    "\n",
    "#Calculate average words per document using for loop\n",
    "sum=0\n",
    "for k,v in abst_dict.items(): #Iterate through dictionary to get value for key\n",
    "    sum=sum+v\n",
    "    average=(sum/len(abst_list))\n",
    "print(\"Average words per document: {0} words/documents\".format(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Total count of 26 alphabets characters in all of the documents\n",
    "We are going to reach each file, and load abstract. Split the words and calculate alphabet frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 2005\n",
      "B 449\n",
      "C 757\n",
      "D 1189\n",
      "E 2401\n",
      "F 176\n",
      "G 348\n",
      "H 670\n",
      "I 2093\n",
      "J 188\n",
      "K 80\n",
      "L 960\n",
      "M 1225\n",
      "N 1691\n",
      "O 1228\n",
      "P 822\n",
      "Q 99\n",
      "R 1265\n",
      "S 1108\n",
      "T 1766\n",
      "U 1050\n",
      "V 119\n",
      "W 53\n",
      "X 15\n",
      "Y 630\n",
      "Z 40\n"
     ]
    }
   ],
   "source": [
    "#Assign 26 alphabets\n",
    "alphabets = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "#Create a new dictionary with keys from alphabets and value from 0\n",
    "alphabet_dict=dict.fromkeys(alphabets, 0)\n",
    "\n",
    "# Using for loop to go through each file to extract the information\n",
    "for file in abst_list:\n",
    "    with open(abst_directory + file, \"r\") as f: #Use of with open to read text file\n",
    "        for word in f.read().split():           #Using for loop to go through list of words\n",
    "            word=word.upper()                   #Convert all the words to uppercase\n",
    "            for letter in word:                 #Usng for loop to go through each character of word\n",
    "                if letter in alphabet_dict:     #If letter is in the dictionary, then add +1 to previous key-value pair\n",
    "                    alphabet_dict[letter] += 1\n",
    "                else:\n",
    "                    alphabet_dict[letter] = 1   #If letter isnot in the dictionary, then assign +1 as a key-value pair\n",
    "\n",
    "#Using for loop to count value of each key in the dictionary\n",
    "for alphabet, frequency in alphabet_dict.items():\n",
    "    if alphabet in alphabets:\n",
    "        print(\"{0} {1}\" . format(alphabet, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top 10 rarely used words-sorted from least used word to most used word\n",
    "We are going to reach each file, and load abstract. Split the words, sort dictionary in ascending order and print top ten rarely used word from all the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=\"1\">5699153</pmid> -->1\n",
      "85 -->2\n",
      "<year>2008</year> -->3\n",
      "issntype=\"print\">0032-3756</issn> -->4\n",
      "<volume>23</volume> -->5\n",
      "<day>10</day> -->6\n",
      "<month>01</month> -->8\n",
      "ui=\"d006801\" -->9\n",
      "<pubmedarticle> -->10\n",
      "<publicationtype -->11\n"
     ]
    }
   ],
   "source": [
    "# Create global variable-empty dictionary for all the key-value pairs from the abstract and unique value-key dictionary\n",
    "word_dict= { }    \n",
    "unique_dict= { }\n",
    "\n",
    "# Using for loop to go through each file to extract the information\n",
    "for file in abst_list:\n",
    "    with open(abst_directory + file, \"r\") as f: #Use of with open to read text file\n",
    "        for word in f.read().split():           #Using for loop to go through list of words\n",
    "            word=word.lower()                   #Convert all the words to lowercase\n",
    "            if word in word_dict:               #If word is in the dictionary, then add +1 to previous key-value pair\n",
    "                word_dict[word] += 1\n",
    "            else:                               #If letter isnot in the dictionary, then assign +1 as a key-value pair\n",
    "                word_dict[word] = 1\n",
    "\n",
    "#Using for loop to extract unique value-key pair from word_dict\n",
    "for k, v in word_dict.items():                  # Create tupple pair from word_dict\n",
    "    if v in unique_dict.values():               # If value is in unique_dict, then go for next tupple and extract key-value\n",
    "        pass\n",
    "    else:\n",
    "        unique_dict[k]=v\n",
    "\n",
    "#Sorting unique_dict in ascending value order and print 10 key-value pair\n",
    "for word in sorted(unique_dict, key=unique_dict.get, reverse=False)[:10]:\n",
    "    print (\"{0} -->{1}\".format(word, unique_dict[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
